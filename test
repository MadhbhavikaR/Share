Let's start by building the class that will handle the workflow file upload and then dynamically apply the new workflow. This class will expose the REST endpoint for file upload and update the workflow configuration accordingly.

Here's how we can approach this step:

Step 1: Add Dependencies to build.gradle

For the workflow functionality, Kafka integration, and file handling, we will need the following dependencies in the build.gradle file:

plugins {
    id 'org.springframework.boot' version '3.1.1'
    id 'io.spring.dependency-management' version '1.1.0'
    id 'java'
}

group = 'com.example'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '17'

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation 'org.springframework.boot:spring-boot-starter-activiti'
    implementation 'org.springframework.kafka:spring-kafka'
    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
    runtimeOnly 'com.h2database:h2'
    
    testImplementation 'org.springframework.boot:spring-boot-starter-test'
    testImplementation 'org.springframework.kafka:spring-kafka-test'
}

spring-boot-starter-web: For creating REST endpoints.

spring-boot-starter-activiti: For managing workflows.

spring-kafka: For Kafka integration.

spring-boot-starter-data-jpa: For database interactions (we will store configurations at the org/job/catalogue level).

h2database: For a simple in-memory database (you can change this to any other DB later).


Step 2: Implement File Upload Controller

We'll now build the class that handles the file upload:

WorkflowFileController.java

package com.example.workflow.controller;

import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

@RestController
@RequestMapping("/api/workflow")
public class WorkflowFileController {

    private static final String WORKFLOW_DIR = "/tmp/workflows/";
    private final KafkaTemplate<String, String> kafkaTemplate;

    @Autowired
    public WorkflowFileController(KafkaTemplate<String, String> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    @PostMapping("/upload")
    public ResponseEntity<String> uploadWorkflowFile(@RequestParam("file") MultipartFile file) {
        try {
            // Ensure the directory exists
            Path workflowPath = Paths.get(WORKFLOW_DIR);
            if (!Files.exists(workflowPath)) {
                Files.createDirectories(workflowPath);
            }

            // Save the uploaded file to the directory
            String fileName = file.getOriginalFilename();
            Path filePath = workflowPath.resolve(fileName);
            Files.write(filePath, file.getBytes());

            // Send a message to Kafka that a new workflow has been uploaded
            kafkaTemplate.send("workflow-updates", "New workflow uploaded: " + fileName);

            // Return success response
            return new ResponseEntity<>("File uploaded successfully", HttpStatus.OK);
        } catch (IOException e) {
            return new ResponseEntity<>("File upload failed: " + e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }
}

Explanation:

1. REST Endpoint:

POST /api/workflow/upload accepts a multipart file upload.

It saves the workflow file to a directory (/tmp/workflows/ for now, but you can change this).



2. Kafka Integration:

After the file is uploaded, a message is sent to a Kafka topic (workflow-updates), informing that a new workflow file has been uploaded.



3. Response Handling: The method returns a success message upon file upload or an error message if something goes wrong.



Step 3: Application Configuration

You'll need to configure Kafka and any other relevant properties in your application.properties or application.yml.

application.properties

spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=workflow-group
spring.kafka.template.default-topic=workflow-updates

Make sure that Kafka is running and the workflow-updates topic is created (you can use Kafka CLI tools to create the topic).

Next Steps:

1. Implement the service that will process the uploaded workflow file.


2. Configure how the new workflow will be applied to future jobs/catalogues.


3. Set up database configurations (e.g., org-level job configurations).



Let me know if this makes sense, and we can proceed with the next class or functionality!



========================================

Now that we have the file upload controller, the next step is to create a service that processes the uploaded workflow file and ensures it's applied to future jobs/catalogues.

We'll approach this by:

1. Processing the Uploaded Workflow File: When a new workflow is uploaded, this service will load the workflow definition and replace the current one.


2. Managing Workflow Configurations: This will involve persisting configurations for org/job/catalogue-level settings in the database.


3. Integrating with Activiti: Once the workflow file is uploaded, Activiti will be notified to use the new workflow for any upcoming jobs.



Let's first build the service responsible for managing the workflow.

Step 1: Create the Workflow Service

WorkflowService.java

package com.example.workflow.service;

import org.activiti.engine.RepositoryService;
import org.activiti.engine.repository.Deployment;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;

@Service
public class WorkflowService {

    private final RepositoryService repositoryService;

    public WorkflowService(RepositoryService repositoryService) {
        this.repositoryService = repositoryService;
    }

    @Transactional
    public String deployWorkflow(File workflowFile) {
        try (FileInputStream fileInputStream = new FileInputStream(workflowFile)) {
            // Deploy the new workflow
            Deployment deployment = repositoryService.createDeployment()
                    .addInputStream(workflowFile.getName(), fileInputStream)
                    .deploy();

            return "Workflow deployed with ID: " + deployment.getId();
        } catch (IOException e) {
            throw new RuntimeException("Failed to deploy workflow", e);
        }
    }
}

Explanation:

1. RepositoryService: This is provided by Activiti and is used to manage deployments. The uploaded workflow file is deployed through this service.


2. DeployWorkflow Method:

It accepts a File object (the uploaded workflow file).

The file is deployed to Activiti using the RepositoryService, which registers the workflow for future jobs.




Step 2: Update the Controller to Use WorkflowService

We will now modify the WorkflowFileController to use the WorkflowService for processing the uploaded workflow.

Updated WorkflowFileController.java

package com.example.workflow.controller;

import com.example.workflow.service.WorkflowService;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

@RestController
@RequestMapping("/api/workflow")
public class WorkflowFileController {

    private static final String WORKFLOW_DIR = "/tmp/workflows/";
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final WorkflowService workflowService;

    @Autowired
    public WorkflowFileController(KafkaTemplate<String, String> kafkaTemplate, WorkflowService workflowService) {
        this.kafkaTemplate = kafkaTemplate;
        this.workflowService = workflowService;
    }

    @PostMapping("/upload")
    public ResponseEntity<String> uploadWorkflowFile(@RequestParam("file") MultipartFile file) {
        try {
            // Ensure the directory exists
            Path workflowPath = Paths.get(WORKFLOW_DIR);
            if (!Files.exists(workflowPath)) {
                Files.createDirectories(workflowPath);
            }

            // Save the uploaded file to the directory
            String fileName = file.getOriginalFilename();
            Path filePath = workflowPath.resolve(fileName);
            Files.write(filePath, file.getBytes());

            // Deploy the workflow using WorkflowService
            String deploymentResult = workflowService.deployWorkflow(filePath.toFile());

            // Send a message to Kafka that a new workflow has been uploaded
            kafkaTemplate.send("workflow-updates", "New workflow uploaded and deployed: " + fileName);

            // Return success response
            return new ResponseEntity<>(deploymentResult, HttpStatus.OK);
        } catch (IOException e) {
            return new ResponseEntity<>("File upload failed: " + e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }
}

Explanation:

1. WorkflowService Integration:

The WorkflowService is injected into the controller.

After the file is uploaded, it's passed to the WorkflowService to deploy the new workflow.



2. Kafka Notification: A Kafka message is sent informing that the workflow has not only been uploaded but also deployed.



Step 3: Database Configuration for Workflow Storage

In future steps, we will implement a configuration layer in the database, allowing workflows to be associated with specific organizations, jobs, or catalogues.

We'll need to:

1. Set up JPA entities and repositories.


2. Persist workflow configurations at the organization/job level.



Next Steps:

Would you like to continue by setting up database storage (configurations for organizations or jobs), or should we focus on Kafka integration for workflow state updates?


=========================


Let's proceed with adding database storage for persisting workflow configurations. This will allow us to associate workflows with specific organizations, jobs, or catalogues. We'll use JPA for this.

Step 1: Define the JPA Entity for Workflow Configuration

We'll create an entity called WorkflowConfig to store configurations per organization or job/catalogue.

WorkflowConfig.java

package com.example.workflow.model;

import jakarta.persistence.*;

@Entity
@Table(name = "workflow_config")
public class WorkflowConfig {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "organization_id", nullable = false)
    private Long organizationId;

    @Column(name = "job_catalogue_id", nullable = false)
    private Long jobCatalogueId;

    @Column(name = "workflow_file_name", nullable = false)
    private String workflowFileName;

    @Column(name = "active", nullable = false)
    private boolean active;

    // Constructors, Getters, and Setters
    public WorkflowConfig() {
    }

    public WorkflowConfig(Long organizationId, Long jobCatalogueId, String workflowFileName, boolean active) {
        this.organizationId = organizationId;
        this.jobCatalogueId = jobCatalogueId;
        this.workflowFileName = workflowFileName;
        this.active = active;
    }

    public Long getId() {
        return id;
    }

    public void setId(Long id) {
        this.id = id;
    }

    public Long getOrganizationId() {
        return organizationId;
    }

    public void setOrganizationId(Long organizationId) {
        this.organizationId = organizationId;
    }

    public Long getJobCatalogueId() {
        return jobCatalogueId;
    }

    public void setJobCatalogueId(Long jobCatalogueId) {
        this.jobCatalogueId = jobCatalogueId;
    }

    public String getWorkflowFileName() {
        return workflowFileName;
    }

    public void setWorkflowFileName(String workflowFileName) {
        this.workflowFileName = workflowFileName;
    }

    public boolean isActive() {
        return active;
    }

    public void setActive(boolean active) {
        this.active = active;
    }
}

Explanation:

1. Entity Structure:

The WorkflowConfig entity represents the configuration of a workflow for an organization and a specific job/catalogue.

It contains fields like organizationId, jobCatalogueId, workflowFileName, and active status to track whether the workflow is currently in use.



2. Persistence:

The entity will be saved to a workflow_config table in the database.




Step 2: Create the JPA Repository

We will now create a repository for WorkflowConfig to handle the database interactions.

WorkflowConfigRepository.java

package com.example.workflow.repository;

import com.example.workflow.model.WorkflowConfig;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;

@Repository
public interface WorkflowConfigRepository extends JpaRepository<WorkflowConfig, Long> {

    List<WorkflowConfig> findByOrganizationIdAndJobCatalogueIdAndActive(Long organizationId, Long jobCatalogueId, boolean active);

    WorkflowConfig findByWorkflowFileNameAndActive(String workflowFileName, boolean active);
}

Explanation:

1. Custom Query Methods:

findByOrganizationIdAndJobCatalogueIdAndActive: Finds an active workflow configuration for a specific organization and job/catalogue.

findByWorkflowFileNameAndActive: Finds an active workflow configuration by its file name.




Step 3: Update WorkflowService to Manage Configurations

We'll update the WorkflowService to persist workflow configurations whenever a new workflow is uploaded and deployed.

Updated WorkflowService.java

package com.example.workflow.service;

import com.example.workflow.model.WorkflowConfig;
import com.example.workflow.repository.WorkflowConfigRepository;
import org.activiti.engine.RepositoryService;
import org.activiti.engine.repository.Deployment;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;

@Service
public class WorkflowService {

    private final RepositoryService repositoryService;
    private final WorkflowConfigRepository workflowConfigRepository;

    public WorkflowService(RepositoryService repositoryService, WorkflowConfigRepository workflowConfigRepository) {
        this.repositoryService = repositoryService;
        this.workflowConfigRepository = workflowConfigRepository;
    }

    @Transactional
    public String deployWorkflow(File workflowFile, Long organizationId, Long jobCatalogueId) {
        try (FileInputStream fileInputStream = new FileInputStream(workflowFile)) {
            // Deploy the new workflow
            Deployment deployment = repositoryService.createDeployment()
                    .addInputStream(workflowFile.getName(), fileInputStream)
                    .deploy();

            // Save workflow configuration in DB
            WorkflowConfig workflowConfig = new WorkflowConfig(organizationId, jobCatalogueId, workflowFile.getName(), true);
            workflowConfigRepository.save(workflowConfig);

            return "Workflow deployed with ID: " + deployment.getId();
        } catch (IOException e) {
            throw new RuntimeException("Failed to deploy workflow", e);
        }
    }
}

Explanation:

1. Database Insertion:

When a new workflow is deployed, the corresponding configuration (organization, job/catalogue ID, workflow file name, and active status) is saved to the database using the WorkflowConfigRepository.



2. Organization and Job/Catalogue Management:

The service now takes organizationId and jobCatalogueId as parameters to associate the workflow with the correct entity.




Step 4: Update the Controller to Handle Organization/Job Input

We will update the WorkflowFileController to pass the organization and job/catalogue ID when calling the service.

Updated WorkflowFileController.java

package com.example.workflow.controller;

import com.example.workflow.service.WorkflowService;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

@RestController
@RequestMapping("/api/workflow")
public class WorkflowFileController {

    private static final String WORKFLOW_DIR = "/tmp/workflows/";
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final WorkflowService workflowService;

    @Autowired
    public WorkflowFileController(KafkaTemplate<String, String> kafkaTemplate, WorkflowService workflowService) {
        this.kafkaTemplate = kafkaTemplate;
        this.workflowService = workflowService;
    }

    @PostMapping("/upload")
    public ResponseEntity<String> uploadWorkflowFile(
            @RequestParam("file") MultipartFile file,
            @RequestParam("organizationId") Long organizationId,
            @RequestParam("jobCatalogueId") Long jobCatalogueId) {
        try {
            // Ensure the directory exists
            Path workflowPath = Paths.get(WORKFLOW_DIR);
            if (!Files.exists(workflowPath)) {
                Files.createDirectories(workflowPath);
            }

            // Save the uploaded file to the directory
            String fileName = file.getOriginalFilename();
            Path filePath = workflowPath.resolve(fileName);
            Files.write(filePath, file.getBytes());

            // Deploy the workflow using WorkflowService
            String deploymentResult = workflowService.deployWorkflow(filePath.toFile(), organizationId, jobCatalogueId);

            // Send a message to Kafka that a new workflow has been uploaded
            kafkaTemplate.send("workflow-updates", "New workflow uploaded and deployed: " + fileName);

            // Return success response
            return new ResponseEntity<>(deploymentResult, HttpStatus.OK);
        } catch (IOException e) {
            return new ResponseEntity<>("File upload failed: " + e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }
}

Explanation:

1. Organization/Job Parameters:

The endpoint now takes organizationId and jobCatalogueId as input parameters, so the workflow can be deployed for a specific organization and job/catalogue.




Next Steps:

1. Do you want to proceed with setting up the consumer logic, or should we further refine the database interactions for retrieving workflows and handling updates dynamically?



==========================


Now that we've implemented the workflow file upload, deployment, and configuration persistence, the next step is to set up the consumer logic for handling workflow state changes and messages from Kafka.

We'll create a Kafka consumer that listens to specific topics and updates the workflow state in the database or performs necessary actions like sending notifications or escalating tasks.

Step 1: Define the Kafka Consumer

We will now create a Kafka consumer service that will listen to a specific Kafka topic for messages related to workflow state changes.

WorkflowStateConsumer.java

package com.example.workflow.kafka;

import com.example.workflow.model.WorkflowConfig;
import com.example.workflow.repository.WorkflowConfigRepository;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.Optional;

@Service
public class WorkflowStateConsumer {

    private final WorkflowConfigRepository workflowConfigRepository;

    public WorkflowStateConsumer(WorkflowConfigRepository workflowConfigRepository) {
        this.workflowConfigRepository = workflowConfigRepository;
    }

    @KafkaListener(topics = "workflow-state-changes", groupId = "workflow-group")
    @Transactional
    public void consumeWorkflowStateChange(ConsumerRecord<String, String> record) {
        String key = record.key();  // e.g., jobCatalogueId
        String value = record.value();  // e.g., new state like "Approved" or "Escalated"

        // Parse the key to extract jobCatalogueId or other identifiers
        Long jobCatalogueId = Long.parseLong(key);

        // Fetch the active workflow configuration for this jobCatalogueId
        Optional<WorkflowConfig> workflowConfigOpt = workflowConfigRepository.findByOrganizationIdAndJobCatalogueIdAndActive(
                1L,  // Placeholder organization ID (adjust based on your setup)
                jobCatalogueId,
                true
        );

        if (workflowConfigOpt.isPresent()) {
            WorkflowConfig workflowConfig = workflowConfigOpt.get();
            
            // Log or handle state change (you can notify, escalate, etc. based on the state)
            System.out.println("Received state change: " + value + " for jobCatalogueId: " + jobCatalogueId);

            // Here you can add logic to handle specific state transitions (e.g., send notification, trigger escalation, etc.)
            if ("Escalated".equals(value)) {
                // Implement escalation handling
                handleEscalation(jobCatalogueId, workflowConfig);
            } else if ("Approved".equals(value)) {
                // Handle approval
                handleApproval(jobCatalogueId, workflowConfig);
            }
            // Add other states as needed
        } else {
            System.err.println("No active workflow configuration found for jobCatalogueId: " + jobCatalogueId);
        }
    }

    private void handleEscalation(Long jobCatalogueId, WorkflowConfig workflowConfig) {
        // Logic for handling escalation
        System.out.println("Handling escalation for jobCatalogueId: " + jobCatalogueId);
        // Notify users, trigger escalation actions, etc.
    }

    private void handleApproval(Long jobCatalogueId, WorkflowConfig workflowConfig) {
        // Logic for handling approval
        System.out.println("Handling approval for jobCatalogueId: " + jobCatalogueId);
        // Notify users, trigger production state, etc.
    }
}

Explanation:

1. Kafka Listener:

The @KafkaListener annotation is used to define a method that listens to the workflow-state-changes topic.

The consumer listens for messages where the key could represent the jobCatalogueId, and the value could represent the new state (e.g., "Approved", "Escalated").



2. Handling State Changes:

When a message is consumed, the application looks up the active workflow configuration for the job/catalogue and performs appropriate actions based on the new state (e.g., escalation, approval).

Custom methods (handleEscalation, handleApproval) are added to perform necessary tasks like notifying users or updating the state.




Step 2: Update Kafka Configuration for Consumer

We need to ensure that our application is configured correctly to consume Kafka messages. Here's how you would configure the application.properties file:

application.properties

spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=workflow-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true

Explanation:

1. Bootstrap Servers: This points to the Kafka broker. If you're running Kafka locally, it will be localhost:9092. Modify this to match your Kafka setup.


2. Consumer Group: The consumer is part of the workflow-group, meaning it will work in coordination with other consumers in the same group (if any).


3. Offset Reset: We use earliest so that the consumer will read from the beginning of the topic if it hasn't processed messages yet.



Step 3: Test Data in Kafka

To test the consumer, we can manually produce some messages into the workflow-state-changes topic. Here’s an example of how you can use the Kafka CLI to produce test data:

1. Produce a message for jobCatalogueId = 1 with the state Escalated:



kafka-console-producer --broker-list localhost:9092 --topic workflow-state-changes
>1,Escalated

2. Produce a message for jobCatalogueId = 2 with the state Approved:



kafka-console-producer --broker-list localhost:9092 --topic workflow-state-changes
>2,Approved

These messages will be consumed by the WorkflowStateConsumer, and the application will handle the respective state changes.

Next Steps:

If you'd like, we can proceed with adding more specific handling for each state transition (e.g., production cooldown, rework).

Alternatively, we can expand the notification system (e.g., sending email or alerting users via external services) for each state change.


Would you like to proceed with specific state handling, or should we expand the notification or persistence features?


==============================

Next, we will focus on improving the state handling mechanism. Specifically, we'll introduce persistence of workflow state transitions to the database and integrate this with audit logging via Kafka.

Step 1: Define the WorkflowStateTransition Entity

We will create an entity to represent the state transitions of workflows. This will allow us to store each transition in the database for future reference, auditing, and rollback purposes.

WorkflowStateTransition.java

package com.example.workflow.model;

import javax.persistence.*;
import java.time.LocalDateTime;

@Entity
@Table(name = "workflow_state_transition")
public class WorkflowStateTransition {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "job_catalogue_id", nullable = false)
    private Long jobCatalogueId;

    @Column(name = "previous_state")
    private String previousState;

    @Column(name = "new_state", nullable = false)
    private String newState;

    @Column(name = "transition_timestamp", nullable = false)
    private LocalDateTime transitionTimestamp;

    @Column(name = "comment", length = 512)
    private String comment;

    public WorkflowStateTransition() {}

    public WorkflowStateTransition(Long jobCatalogueId, String previousState, String newState, LocalDateTime transitionTimestamp, String comment) {
        this.jobCatalogueId = jobCatalogueId;
        this.previousState = previousState;
        this.newState = newState;
        this.transitionTimestamp = transitionTimestamp;
        this.comment = comment;
    }

    // Getters and setters...

    public Long getId() {
        return id;
    }

    public Long getJobCatalogueId() {
        return jobCatalogueId;
    }

    public void setJobCatalogueId(Long jobCatalogueId) {
        this.jobCatalogueId = jobCatalogueId;
    }

    public String getPreviousState() {
        return previousState;
    }

    public void setPreviousState(String previousState) {
        this.previousState = previousState;
    }

    public String getNewState() {
        return newState;
    }

    public void setNewState(String newState) {
        this.newState = newState;
    }

    public LocalDateTime getTransitionTimestamp() {
        return transitionTimestamp;
    }

    public void setTransitionTimestamp(LocalDateTime transitionTimestamp) {
        this.transitionTimestamp = transitionTimestamp;
    }

    public String getComment() {
        return comment;
    }

    public void setComment(String comment) {
        this.comment = comment;
    }
}

Step 2: Create the WorkflowStateTransitionRepository

This repository will handle the interaction with the database for the WorkflowStateTransition entity.

WorkflowStateTransitionRepository.java

package com.example.workflow.repository;

import com.example.workflow.model.WorkflowStateTransition;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import java.util.List;

@Repository
public interface WorkflowStateTransitionRepository extends JpaRepository<WorkflowStateTransition, Long> {
    List<WorkflowStateTransition> findByJobCatalogueId(Long jobCatalogueId);
}

Step 3: Modify the Kafka Consumer to Persist Transitions

We'll now modify the WorkflowStateConsumer to persist the workflow state transitions. We will also add logic to publish audit logs to a Kafka topic.

Modified WorkflowStateConsumer.java

package com.example.workflow.kafka;

import com.example.workflow.model.WorkflowConfig;
import com.example.workflow.model.WorkflowStateTransition;
import com.example.workflow.repository.WorkflowConfigRepository;
import com.example.workflow.repository.WorkflowStateTransitionRepository;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.Optional;

@Service
public class WorkflowStateConsumer {

    private final WorkflowConfigRepository workflowConfigRepository;
    private final WorkflowStateTransitionRepository stateTransitionRepository;
    private final KafkaTemplate<String, String> kafkaTemplate;

    public WorkflowStateConsumer(WorkflowConfigRepository workflowConfigRepository,
                                 WorkflowStateTransitionRepository stateTransitionRepository,
                                 KafkaTemplate<String, String> kafkaTemplate) {
        this.workflowConfigRepository = workflowConfigRepository;
        this.stateTransitionRepository = stateTransitionRepository;
        this.kafkaTemplate = kafkaTemplate;
    }

    @KafkaListener(topics = "workflow-state-changes", groupId = "workflow-group")
    @Transactional
    public void consumeWorkflowStateChange(ConsumerRecord<String, String> record) {
        String key = record.key();  // e.g., jobCatalogueId
        String value = record.value();  // e.g., new state like "Approved" or "Escalated"
        String[] messageParts = value.split(",");
        String newState = messageParts[0];
        String comment = messageParts.length > 1 ? messageParts[1] : null;

        Long jobCatalogueId = Long.parseLong(key);

        // Fetch the active workflow configuration for this jobCatalogueId
        Optional<WorkflowConfig> workflowConfigOpt = workflowConfigRepository.findByOrganizationIdAndJobCatalogueIdAndActive(
                1L,  // Placeholder organization ID
                jobCatalogueId,
                true
        );

        if (workflowConfigOpt.isPresent()) {
            WorkflowConfig workflowConfig = workflowConfigOpt.get();

            String previousState = workflowConfig.getCurrentState();
            workflowConfig.setCurrentState(newState);
            workflowConfigRepository.save(workflowConfig);

            // Save state transition
            WorkflowStateTransition stateTransition = new WorkflowStateTransition(
                    jobCatalogueId, previousState, newState, LocalDateTime.now(), comment
            );
            stateTransitionRepository.save(stateTransition);

            // Send audit log to Kafka
            sendAuditLog(jobCatalogueId, previousState, newState);

            System.out.println("State changed from " + previousState + " to " + newState + " for jobCatalogueId: " + jobCatalogueId);
        } else {
            System.err.println("No active workflow configuration found for jobCatalogueId: " + jobCatalogueId);
        }
    }

    private void sendAuditLog(Long jobCatalogueId, String previousState, String newState) {
        String auditMessage = "Workflow state changed for jobCatalogueId: " + jobCatalogueId +
                " from " + previousState + " to " + newState;
        kafkaTemplate.send("workflow-audit-logs", jobCatalogueId.toString(), auditMessage);
    }
}

Explanation:

1. Persisting Workflow State Transitions:

For each state change, a new entry is added to the workflow_state_transition table.

This allows tracking of all state transitions for auditing purposes.



2. Audit Logging:

After a successful state transition, an audit log is created and sent to a Kafka topic (workflow-audit-logs).

The KafkaTemplate is used to send the audit log message to Kafka.



3. Handling Comments:

The state change message from Kafka is assumed to potentially contain a comment (e.g., for Declined or Escalated states).

This comment is optional and is saved along with the state transition.




Step 4: Update Kafka Producer for Audit Logs

We need to ensure that we have Kafka configured for the audit logs. In the application.properties:

spring.kafka.producer.bootstrap-servers=localhost:9092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

This will enable Kafka to produce audit logs.

Step 5: Testing

You can test the functionality by:

1. Producing a message for state change (with a comment) using Kafka CLI:

kafka-console-producer --broker-list localhost:9092 --topic workflow-state-changes
>1,Escalated,Please respond within 24 hours


2. Verifying the state transition in the workflow_state_transition table by querying the database.


3. Checking the audit logs sent to the workflow-audit-logs topic by consuming the messages:

kafka-console-consumer --bootstrap-server localhost:9092 --topic workflow-audit-logs --from-beginning



Next Steps:

We can now:

1. Implement notification services to alert relevant users when certain state changes occur.


2. Handle more advanced state transition flows (e.g., cooldown periods, automated escalation).


3. Set up testing and monitoring for Kafka consumers and producers.



Would you like to proceed with notifications or implement advanced state flows?







